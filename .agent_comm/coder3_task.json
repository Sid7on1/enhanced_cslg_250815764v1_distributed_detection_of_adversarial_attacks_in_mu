{
  "agent_id": "coder3",
  "task_id": "task_3",
  "files": [
    {
      "name": "memory.py",
      "purpose": "Experience replay and memory",
      "priority": "medium"
    },
    {
      "name": "reward_system.py",
      "purpose": "Reward calculation and shaping",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.LG_2508.15764v1_Distributed_Detection_of_Adversarial_Attacks_in_Mu",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.LG_2508.15764v1_Distributed-Detection-of-Adversarial-Attacks-in-Mu with content analysis. Detected project type: agent (confidence score: 15 matches).",
    "key_algorithms": [
      "Tection",
      "Such",
      "Trained",
      "Machine",
      "Distributed",
      "Learning",
      "Enhances",
      "Posed",
      "Ppo",
      "Ddpg"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.LG_2508.15764v1_Distributed-Detection-of-Adversarial-Attacks-in-Mu.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nDistributed Detection of Adversarial Attacks in\nMulti-Agent Reinforcement Learning with Continuous\nAction Space\nKiarash Kazaria,*, Ezzeldin Shereenaand Gy\u00f6rgy D\u00e1na\naKTH Royal Institute of Technology, Stockholm, Sweden\nAbstract. We address the problem of detecting adversarial attacks\nagainst cooperative multi-agent reinforcement learning with contin-\nuous action space. We propose a decentralized detector that relies\nsolely on the local observations of the agents and makes use of\na statistical characterization of the normal behavior of observable\nagents. The proposed detector utilizes deep neural networks to ap-\nproximate the normal behavior of agents as parametric multivariate\nGaussian distributions. Based on the predicted density functions, we\ndefine a normality score and provide a characterization of its mean\nand variance. This characterization allows us to employ a two-sided\nCUSUM procedure for detecting deviations of the normality score\nfrom its mean, serving as a detector of anomalous behavior in real-\ntime. We evaluate our scheme on various multi-agent PettingZoo\nbenchmarks against different state-of-the-art attack methods, and our\nresults demonstrate the effectiveness of our method in detecting im-\npactful adversarial attacks. Particularly, it outperforms the discrete\ncounterpart by achieving AUC-ROC scores of over 0.95 against the\nmost impactful attacks in all evaluated environments.\n1 Introduction\nIn recent years we have witnessed enormous success for multi-agent\nreinforcement learning (MARL) algorithms, with potential applica-\ntions in autonomous driving, 5G networks, robotics, and smart grid\ncontrol [3]. Based on sequences of observations and rewards from\nthe environment, cooperative MARL (c-MARL) enables a group of\nagents to perform complex sequential tasks by learning an optimal\ndistributed policy. Besides enabling distributed execution of tasks, c-\nMARL has been shown to consistently outperform centralized and\nsingle-agent RL approaches, especially for complex tasks [3].\nDespite the apparent success of c-MARL, its adoption in safety-\ncritical applications will be dependent on its resilience to noise,\nfaults, and adversarial manipulations. In particular, motivated by the\nrise of adversarial evasion attacks in machine learning and computer\nvision [9], the trustworthiness of seemingly high-performing data-\ndriven algorithms is becoming increasingly important. In the con-\ntext of c-MARL for example, an adversary that could compromise\nan agent could cause it to take sub-optimal actions (either through\ndirect manipulation, or indirectly through manipulating its observa-\ntions), so as to minimize the team reward [19]. One approach to ad-\ndress adversarial manipulations in MARL is to improve its resilience,\n\u2217Corresponding Author. Email: kkazari@kth.se.e.g., using robust training through dataset augmentation or adversar-\nial regularization [2]. Nonetheless, even though this can mitigate the\neffect of adversarial manipulations to some extent, it does not pro-\nvide situational awareness.\nSituational awareness requires the timely detection of adversarial\nactivity, and allows to remove or replace misbehaving agents in real-\ntime, or to adjust the policy of non-compromised agents. Existing\nanomaly detection schemes for single-agent RL [31, 45, 12, 38] are\ncentralized and primarily focus on self-diagnosis by detecting manip-\nulations in the sequence of observations or states. However, in many\nMARL applications, the agents\u2019 observations are private, hence these\napproaches are ineffective, especially against a powerful adversary\ncapable of not only manipulating observations but also the victim\u2019s\nactions. Moreover, applying these approaches in a centralized man-\nner to MARL would be limited to detecting only the presence of an\nattack, without being able to identify the specific victim.\nDecentralized detection schemes for MARL, on the other hand, as-\nsume that the agents\u2019 action spaces are discrete and assign probabil-\nities to all actions of the agents [21, 15]. However, many real-world\napplications of c-MARL involve agents operating in continuous ac-\ntion spaces. For instance, in robotics, agents often need to control\nmotors or actuators with continuous control signals [3], or need to\nset voltage and frequency for the control of inverter-based resources\nin smart grids [41]. Adapting discrete-action schemes to continuous\naction spaces would require one to quantize the action space. Impor-\ntantly, achieving acceptable detection accuracy would demand many\nquantization levels per action dimension; otherwise, critical infor-\nmation about the structure and different classes of actions in contin-\nuous space can be lost [46]. The alternative, assuming independence\namong different action dimensions is a strong assumption that of-\nten does not hold [5]. Thus, fine-grained quantization is necessary,\nbut it leads to a complexity growing exponentially with the number\nof action dimensions. As a result, in high-dimensional action spaces,\nwhich are common in applications such as robotic control [29], exist-\ning detection methods become effectively impractical. Instead, a low-\ncomplexity method is needed specifically designed for MARL with\ncontinuous action spaces to detect adversarial attacks effectively.\nIn this paper, we propose a decentralized approach for detecting\nadversarial attacks in model-free c-MARL with continuous action\nspace. Our contributions are as follows:\n1. We utilize the observations of agents to train neural networks that\npredict the actions of other agents as parameterized multivariate\nGaussian distributions.arXiv:2508.15764v1  [cs.LG]  21 Aug 2025\n\n--- Page 2 ---\n2. We use the predicted distribution for computing a normality score,\nwhich quantifies how well individual agents conform to their pre-\ndicted behavior. We provide an analytical characterization of the\nmean normality score, which allows us to cast attack detection as a\nmean shift detection problem, and we propose to use the CUSUM\nmethod as a solution.\n3. Through extensive simulations, we demonstrate the effectiveness\nof the proposed scheme in detecting state-of-the-art attacks in four\nbenchmark MARL environments with continuous action space.\nFurthermore, we show that our proposed method outperforms the\nstate-of-the-art discrete-action alternative in terms of performance\nand computational complexity.\nThe rest of this paper is organized as follows. Section 2 discusses\nprevious work on adversarial attacks against MARL and counter-\nmeasures. Section 3 presents the system model and formulates the\nproblem of decentralized attack detection. Section 4 presents the pro-\nposed detection approach. The performance of the proposed method\nis evaluated in Section 5. Section 6 concludes the paper.\n2 Related Work\nMany studies addressing adversarial attacks in single-agent rein-\nforcement learning focus on perturbing states or observations. In\nthese studies, the adversary manipulates observations so that the\nagent makes sub-optimal decisions with respect to the actual state\nof the environment [27],[1], [28], [24]. In the context of RL with\ncontinuous action spaces, [34] proposed real-time adversarial state\nmanipulation attacks against Deep RL and showed the effectiveness\nof such attacks in MuJoCo [37] physics-based tasks like Walker2d.\nIn the domain of MARL, [19] considered perturbing the obser-\nvations of a single agent in a c-MARL system. In [6], the authors\nproposed state manipulation adversarial attacks on cluster-based, het-\nerogeneous MARL systems. More powerful attack models in the lit-\nerature attack directly the actions of the agents. In the competitive\nMARL setting, [8] showed that an adversary could deceive an agent\nby manipulating the actions of the competing agent. [10] explored the\nrobustness of state-of-the-art c-MARL algorithms against attacks on\nobservations and actions. In a closely related work, [7] studied the\neffect of adversarial attacks on consensus-based networked MARL\nalgorithms. In a different application domain, [33] investigated the\nvulnerability of MARL-based microgrid voltage control to adversar-\nial attacks against sensor readings. Finally, [44] considered robust-\nness against attacks to messages in communicative MARL.\nRelated to ours are previous works on anomaly detection for se-\nquential data [25], [23],[39]. However, anomaly detection in the con-\ntext of RL has received less attention compared to domains like\nvideo, audio, and text. Among the works addressing anomaly de-\ntection in RL, [45] introduced a framework for detecting anoma-\nlous state observations based on a Gaussian approximation of the\nstate representation space. [31] proposed an entropy-based anomaly\ndetector for identifying out-of-distribution observations, though not\nwithin an adversarial setting. Adversarial detection, i.e., identifying\nthe adversarial samples generated in the state space of deep RL is the\nfocus of [20]. Notably, none of these works addressed anomaly de-\ntection in a multi-agent setting. Applying these single-agent schemes\nto MARL would either necessitate centralized tracking of all agents\nor implementing anomaly detection locally for each agent. The for-\nmer may not be feasible in decentralized multi-agent systems due to\nthe resource-intensive nature of collecting data from all agents. The\nlatter is sub-optimal as it does not account for adversaries with fullcontrol over the actions of the victim agent. In contrast, our approach\ninvolves other agents interacting with the environment for anomaly\ndetection. Regardless of whether the misbehavior of the victim agent\nresults from perturbations in its observations or actions, our method\ncan detect anomalies.\nConcerning multi-agent systems, existing works [32, 43] have pro-\nposed decentralized defenses against adversarial attacks in control\nsystems. However, these approaches rely on a known model of the\nsystem, which is typically not available in MARL applications. Clos-\nest to our work are recent works addressing anomaly detection in\nc-MARL [15, 21]. These works rely on a categorical characteriza-\ntion of the actions taken by agents, making them applicable only to\nMARL problems with discrete action space. On the contrary, in this\nwork we propose a detector that is applicable to MARL problems\nwith continuous action space.\n3 System Model and Problem Formulation\n3.1 System Model\nWe consider a cooperative multi-agent environment modeled by a\ndecentralized partially-observable Markov decision process (Dec-\nPOMDP). The Dec-POMDP can be described by a tuple M=\n(K,S,{Ai}i\u2208K, R, P, {Oi}i\u2208K, \u03b3), where K={1,2, ..., K}is the\nset of agents, Sis the state space, and Oiis the set of observations of\nagent i. We let Ai\u2286Rdibe the continuous action space of agent i.\nMoreover, RandP, denote the reward function, and the state transi-\ntion probability, respectively, and \u03b3\u2208(0,1)is a discount factor. At\neach time step t\u22650, agent iobserves oi\nt\u2208 Oifrom the environment\nand chooses an action ai\nt\u2208 Ai. The joint action profile at={ai\nt}\ncauses the state of the system to change from st=stost+1=s\u2032\nwith probability P(s\u2032|s,at), and a shared reward Rt=R(st,at)is\nreceived by all agents. In the reinforcement learning setting, which is\nthe focus of this work, it is assumed that PandRare unknown func-\ntions. The objective of the agents is to maximize the total discounted\naverage reward, i.e., E[P\u221e\nt=0\u03b3tRt].\nWe say that agent jis an observable neighbor of agent iif agent\nihas access to the sequence of actions played by agent j. Note that\nthis does not necessarily mean that those actions are in oi\nt. Instead,\nin many real-world applications, such as multi-UA V navigation or\nautonomous driving, where other agents\u2019 positions are part of an\nagent\u2019s observation, their past actions can be inferred (i.e., they are\nimplicitly observable). We denote by Kithe set of all observable\nneighbors of agent i. We assume that Kiis constant over time, and\nthat for each agent jthere exists at least one agent isuch that j\u2208 Ki,\nwhich is a prerequisite for distributed detection.\nWe consider that all agents have been trained using a MARL al-\ngorithm, and in the deployment time, agent iacts based on a local\npolicy \u03c0i(\u03c4i\nt), where \u03c4i\nt\u2208\u0393i\u225c(Oi\u00d7Ai)\u2217is the history of action-\nobservations of agent iup to time t, i.e.,\u03c4i\nt= (oi\n0, ai\n0, ..., oi\nt).\u03c0i(\u03c4i\nt)\ncan be a stochastic or a deterministic policy.\n3.2 Attack Model\nWe consider an adversary that can eavesdrop on the observations and\ncan manipulate the actions of one agent v\u2208 K in the deployment\ntime of c-MARL. That is, at each time step t, the adversary can ob-\nserve oadv\nt=ov\ntfrom the environment, and can cause the victim\nagent to take an action aadv\nt\u2208 Avinstead of av\nt, where aadv\ntfollows\nthe adversary\u2019s policy \u03c0adv\u0338=\u03c0v(\u03c4v\nt). We refer to agent vas the vic-\ntim agent. This attack model is in line with the models considered in\n\n--- Page 3 ---\nFigure 1. System model with agent jas victim.\n[10] and [15], and is illustrated in Figure 1. Note that although we as-\nsume that only one agent is compromised, our proposed approach is\napplicable in scenarios involving multiple victims (c.f. Section 5.8).\n3.3 Problem Formulation\nConsider that the adversary starts to attack some agent vat time step\nt0(t0might be considered \u221ein case that no attack happens). Given\nthe sequence of actions of all agents, our objective is to identify the\nvictim agent as soon as possible after the attack starts in a distributed\nmanner, i.e., detection should be done by the agents based on locally\navailable information. Consequently, treating agent ias an observer\nand agent j\u2208 Kias a prospective victim, the considered anomaly\ndetection problem can be formulated as follows.\nDecentralized c-MARL Attack Detection : Given a history \u03c4ij\nt=\noi\n0, aj\n0, oi\n1, aj\n1, ..., oi\nt, aj\ntat time t, find a function Mij: (Oi\u00d7\nAj)\u2217\u2192 {0,1}, s.t.Mij(\u03c4ij\nt) = 0 if the sequence \u03c4ij\ntis normal (i.e.,\nit is the consequence of following \u03c0jby agent j) and Mij(\u03c4ij\nt) = 1\nif\u03c4ij\ntis abnormal (agent jis a victim).\n4 Decentralized Attack Detection\nIn this section, we describe our proposed detector, i.e., the algo-\nrithm for implementing function Mijfor agent i\u2208 K and agent\nj\u2208 Ki. The proposed algorithm is composed of two steps. First,\nit estimates the distribution of agent j\u2019s action (for the upcoming\ntime-step) based on the observations of agent i. In the second step, a\nnormality score is computed based on the estimated distribution and\nthe action actually taken by agent j; the normality score is then used\nfor detection, relying on a characterization of its mean and standard\ndeviation. Next, we present a detailed description of these two steps.\n4.1 Learning Action Distributions\nWe propose to approximate the action distribution fij(.|\u03c4i\nt)of agent\njfrom agent i\u2019s point of view by a dj-dimensional Gaussian dis-\ntribution (recall that djis the dimension of the action space) pa-\nrameterized by its mean, \u00b5ij\nt(\u03c4i\nt)\u2208Rdj, and its covariance matrix,\n\u03a3ij\nt(\u03c4i\nt)\u2208Rdj\u00d7dj. Observe that the parameters of the distribution\nare a function of the history \u03c4i\nt. The proposed approximation is moti-\nvated by that many deep RL algorithms for continuous action spaces\nrepresent a policy as a multivariate Gaussian-distribution [42]. While\nthis does not imply that the actions of other agents based on the ob-\nservation of a single agent would be Gaussian, our results in Section 5show that the Gaussian distribution can effectively approximate the\nother agents\u2019 behaviour based on local observations. The approxima-\ntion can thus be formulated as\nfij(x|\u03c4i\nt)\u2248\n(2\u03c0)\u2212dj\n2det(\u03a3ij\nt)\u22121\n2exp\u0014\n\u22121\n2(x\u2212\u00b5ij\nt)\u22a4(\u03a3ij\nt)\u22121(x\u2212\u00b5ij\nt)\u0015\n=g(x;\u00b5ij\nt,\u03a3ij\nt). (1)\nThe objective of agent iis to learn a function that takes \u03c4i\ntas the\ninput, and outputs \u00b5ij\ntand\u03a3ij\nt. We propose to approximate this func-\ntion using a recurrent neural network (RNN), represented as NETij.\nOur approach is based on the intuition that the task of predicting\nthe parameters of the Gaussian distribution can be regarded as a re-\ngression problem where (the sequence of) agent i\u2019s observations are\nthe inputs. In the training phase of NETij, the objective is to find\nweights \u03b8ijsuch that the outputs, i.e., \u00b5and\u03a3would maximize\nthe log-likelihood of observing aj\ntgiven \u03c4i\nt. Accordingly, NETijis\ntrained to maximize the expected log-likelihood E[logfij(aj\nt|\u03c4i\nt)].\nUsing (1), the objective function can be expanded as\nmax\n\u03b8ijE[logfij(aj\nt|\u03c4i\nt)] = max\n\u03b8ijE[logg(aj\nt;\u00b5ij\nt,\u03a3ij\nt)]\n= max\n\u03b8ijE\u0014\n\u22121\n2\u0010\nlog det( \u03a3ij\nt)\n+ (aj\nt\u2212\u00b5ij\nt)\u22a4(\u03a3ij\nt)\u22121(aj\nt\u2212\u00b5ij\nt)\u0011\u0015\n(2)\nNote that \u03a3ij\ntis a covariance matrix, hence it should be symmet-\nric and positive definite. In order to ensure this condition is met,\nwe consider the Cholesky factorization [14] of the covariance ma-\ntrix as \u03a3ij\nt=Lij\ntLij\u22a4\ntwhere Lij\ntis a lower triangular matrix. Con-\nsequently, we treat the non-zero elements of Lij\ntas the outputs of\nNETij, which are unconstrained. Now, the objective function in (2)\nis equivalent to\nmin\n\u03b8ijE\u0014\n2 log det( Lij\nt)\n+ (aj\nt\u2212\u00b5ij\nt)\u22a4(Lij\ntLij\u22a4\nt)\u22121(aj\nt\u2212\u00b5ij\nt)\u0015\n(3)\n= min\n\u03b8ijE\u0014\n2djX\nk=1loglij\nt[k] +yij\u22a4\ntyij\nt\u0015\n(4)\nwhere lij\nt[k]denotes the k-th diagonal element of Lij\ntandyij\nt\u225c\nLij\u22121\nt(aj\nt\u2212\u00b5ij\nt). Note that since Lij\ntis a lower triangular matrix,\nthere is no need for inverting it and yij\ntcan be computed directly us-\ning low complexity algorithms like forward substitution [14]. Train-\ning NETijis done by collecting samples of the form (\u03c4i\nt, aj\nt)during\nthe normal execution of the policies {\u03c0j}j\u2208Kof the agents, and then\noptimizing the network\u2019s parameters according to the objective (4).\n4.2 Normality Score and Detection Procedure\nWe use the action density functions learned in the previous step for\nquantifying the normality of the sequence of agent j\u2019s actions up\nto time step t. While the problem seemingly resembles that of se-\nquential hypothesis testing [17], where the objective is to determine\nwhether a sequence of samples are generated from a given distri-\nbution, in our case the distribution is state-dependent and changes\n\n--- Page 4 ---\nFigure 2. Proposed detection scheme for agent ias the observer and agent\njas the potential victim.\nover time. Hence, existing approaches to sequential hypothesis test-\ning cannot be applied directly.\nInstead, motivated by the log-likelihood ratio computed in sequen-\ntial hypothesis testing approaches, we propose to use the normal-\nized log-likelihood based on the approximated action distributions to\ncompute a normality score defined as\nzij\nt= log(fij(aj\nt|\u03c4i\nt)\nmax xfij(x|\u03c4i\nt)), (5)\nwhich represents the log-likelihood of the predicted density at aj\ntnor-\nmalized by the maximum value of the density function. Intuitively,\nthe reason for the normalization is to take the confidence of the pre-\ndictions into account.\nImportantly, if fijis Gaussian then zij\ntis closely related to the\nMahalanobis distance of aj\ntandfij(.|\u03c4i\nt). To see why, observe that\nfijattains its maximum at x=\u00b5ij\nt. Thus,\nzij\nt= log\u0014g(aj\nt;\u00b5ij\nt,\u03a3ij\nt)\ng(\u00b5ij\nt;\u00b5ij\nt,\u03a3ij\nt)\u0015\n(6)\n=\u22121\n2(aj\nt\u2212\u00b5ij\nt)\u22a4(\u03a3ij\nt)\u22121(aj\nt\u2212\u00b5ij\nt) =\u2212D(aj\nt, fij\nt)\n2,\nwhere D(., .)denotes the Mahalanobis distance. An essential prop-\nerty of the score we propose for detection is the invariance of its\nexpected value over time.\nProposition 1. Consider agents i, j\u2208 K at time t, where j\u2208 Ki,\nand assume aj\nt|\u03c4i\nt\u223c N (\u00b5ij\nt,\u03a3ij\nt). Then the first and second mo-\nments of zij\ntare\nmj\nz\u225cE[zij\nt] =\u2212dj/2, \u03c3j2\nz\u225cE[(zij\nt\u2212mj\nz)2] =dj/2.(7)\nProof. For ease of notation, we drop the subscript tand superscripts\ni, j. We can write\nE[z] =\u22121\n2E[(a\u2212\u00b5)\u22a4\u03a3\u22121(a\u2212\u00b5)]\n=\u22121\n2E[tr((a\u2212\u00b5)\u22a4\u03a3\u22121(a\u2212\u00b5))]\n=\u22121\n2E[tr(\u03a3\u22121(a\u2212\u00b5)\u22a4(a\u2212\u00b5))]\n=\u22121\n2tr(\u03a3\u22121E[(a\u2212\u00b5)\u22a4(a\u2212\u00b5)])\n=\u22121\n2tr(\u03a3\u22121\u03a3) =\u2212d/2. (8)\nFor the variance, we can write\nE[z2] =1\n4E[((a\u2212\u00b5)\u22a4\u03a3\u22121(a\u2212\u00b5))2]. (9)Since \u03a3is a positive definite matrix, it has a unique symmetric pos-\nitive semi definite square root R, such that RR= \u03a3 . Then for\nx=R\u22121(a\u2212\u00b5)it holds that x\u223c N(0, I). Thus, we have\nE[z2] =1\n4E[(x\u22a4RR\u22121R\u22121Rx)2] =1\n4E[(x\u22a4x)2]\n=1\n4(tr(I)2+ 2tr(I)) =1\n4(d2+ 2d). (10)\nAccordingly, \u03c32\nz=E[z2]\u2212E[z]2=d/2.\nProposition 1 shows that, although the distribution of predicted\nactions varies over time, as long as the actions of agent jfollow the\nconditional distribution expected by observer agent i, the expected\nvalue of the normality score remains constant.\nImportantly, Proposition 1 allows us to cast attack detection as the\nproblem of detecting a shift in the mean of a sequence of random\nvariables, for which CUSUM is a well-known stopping rule [26],\nwhich is optimal under the assumption that the samples are i.i.d\n[16, 40]. In our setting, the samples zij\ntare not independent; how-\never, the complexity of their dependencies makes it infeasible to de-\nrive an optimal stopping rule. We thus propose to apply CUSUM on\nthe normality score zij\ntcombined with Proposition 1 for detection.\nAs shown in Section 5, it performs well despite samples not being\nindependent. Accordingly, our proposed detection method is based\non computing\ncij+\nt= max {0, cij+\nt\u22121+zij\nt\u2212mj\nz\n\u03c3j\nz\u2212w}, (11)\ncij\u2212\nt= max {0, cij\u2212\nt\u22121+mj\nz\u2212zij\nt\n\u03c3j\nz\u2212w}, (12)\nwhich are used to determine a deviation from the mean in the positive\nand the negative directions, receptively, wis a tunable hyperparam-\neter that determines CUSUM\u2019s sensitivity, and cij+\n\u22121=cij\u2212\n\u22121= 0.\nFor detection, each agent icontinuously updates cij+\ntandcij\u2212\ntcor-\nresponding to every agent jinKi, and considers agent jas com-\npromised if either cij+\nt> \u03b2ij+orcij\u2212\nt> \u03b2ij\u2212, where \u03b2ij+and\n\u03b2ij\u2212are predefined thresholds. We refer to the proposed detection\nscheme as the Parameterized Gaussian CUSUM (PGC) detector. Fig-\nure 2 shows the overall structure of the proposed detector.\nThe proposed detector can be combined with different decision\nrules for distributed detection of an attack. A straightforward rule\nwould be to consider an agent as compromised once any of the other\nagents detects it as attacked. Alternatively, one could consider an\nagent compromised when at least u >1agents detect that it is com-\npromised.\n5 Evaluation\nWe present results obtained using the proposed detector against four\ntypes of attacks in four MARL environments.\n5.1 MARL Environments\nWe used four PettingZoo c-MARL continuous action environ-\nments [35] for the evaluation, namely, Multiwalker [11], Tag(Simple\nTag) [22], World Comm [22], and Pistonball . The main properties of\nthe considered scenarios are summarized in Table 1. Based on the\nobservations provided by the environments, in Pistonball and Multi-\nwalker, Kiis the set of the agents adjacent to agent i, while in the\nother two environments, Kiincludes all agents other than i.\n\n--- Page 5 ---\nTable 1. Number of agents, observation size, action space dimension, and\ntime limit in the considered scenarios. In Tag and World Comm one of the\nagents does not belong to the c-MARL team.\nEnvironment Agents Obs. SizeAction\nDim.Time\nLimit\nMultiwalker 5 31 4 200\nTag 4 16 5 25\nWorldComm 5 25 9 25\nPistonball 10 RGB (84,84,3) 1 125\nFigure 3. Structure of NETijused for prediction.\n5.2 Attack Strategies\nWe use four attack strategies for the evaluation.\nRandom Attack (RAND): A naive adversary that at each step\nchooses an action uniformly at random in Av.\nAction Attack (ACT): The adversary\u2019s objective is to minimize\nthe team reward. The policy of the adversary is obtained by solving\na single-agent reinforcement learning problem with the action space\ndefined as Avand the objective of minimizing E[P\u221e\nt=0\u03b3tRt][10].\nSince the policies of the non-victim agents are fixed, they can be\nconsidered part of the environment.\nProjected Gradient-Based Attack (GRAD): This approach is\ncommonly used in single agent RL to manipulate agent\u2019s observa-\ntions [27]. It is based on finding a suitable l\u221e-bounded manipulation\nthrough a perturbation in the gradient-ascent direction of a loss func-\ntion, e.g., as in FGSM [9]. Due to continuity of the action space, we\napply this method directly to the actions. Accordingly, we considered\nthe following manipulation strategy based on the Q function:\naadv\nt=Proj{av\nt\u2212\u03f5sign (\u2207aQv(\u03c4v\nt, av\nt))}, (13)\nwhere Qv(\u03c4, a)is the victim\u2019s Q function, and Proj denotes the\nprojection onto Av.\nDynamic Adversary (DYN): The dynamic adversary is a power-\nful adversary that is aware of the detection scheme and can observe\nthe normality scores that the agents compute. It uses this informa-\ntion for learning attacks that are hard to detect using the proposed\ndetector, as done in [15]. Following the notion of an expectedly un-\ndetectable attack introduced in [15], the adversary\u2019s problem can be\nformulated as a constrained RL problem whose solution would be\na Markovian policy. To tailor this attack against our detection pro-\ncedure, we consider the reward function for this single agent RL as\nRadv\nt=\u2212(Rt+P\ni:v\u2208Ki\u03bbi|ziv\nt\u2212mv\nz|), where \u03bbiis a weight\nparameter that captures the importance of remaining undetected by\nagent i, and effectively controls the trade-off between detectability\nand the impact of the attack on the team reward.\nFor instance, for \u03bbi= 0the attack is equivalent to ACT, maximiz-\ning attack impact. Conversely, for high values of \u03bbi, the objective\nis dominated by trying to remain undetected by keeping the normal-\nity score close to its expectation. For simplicity we consider uniform\nweights, i.e., \u03bbi=\u03bb.5.3 Baselines\nExisting single agent RL anomaly detection methods typically detect\nanomalies in the observations or states of an agent [31, 45]. Such\ndetection schemes could be used by the victim, but if used by non-\nvictim agents, they would not be able to identify the victim agent,\nonly the fact that there may be an attack. To provide a fair compari-\nson, we thus focus on baselines that allow to identify victim agents.\nWe provide a comparison to [45] in Appendix C.4.\nAs baselines we thus use the detection method proposed in [15]\nfor MARL with discrete action spaces, which we refer to as Discrete ,\nand a number of variants of our detection method. To adapt Discrete\nto a continuous action space, we uniformly quantize each dimension\nof the action space into Qintervals, resulting in an action set of size\nQdif the continuous action set has dimension d.\nWe consider three variants of our detection method: using the\nDirichlet distribution, the Beta distribution, and Gaussian with in-\ndependent components (i.e., a diagonal covariance) as the distribu-\ntion approximating fij. These variants are referred to as Dirichlet ,\nBeta, and Independent PGC (I-PGC) in this section. For the Dirich-\nletand Beta baselines, since the normality score is not constant\n(c.f. Proposition 1), CUSUM is not applicable. Instead, we used the\nwindow-average-based method proposed by [15] to compute the de-\ncision metrics. For comparison, we also include results for replacing\nCUSUM with this metric in PGC in Appendix C.3.\n5.4 Evaluation Methodology\nGiven the four environments and four attack strategies, we followed\nfour steps for the evaluation.1\n5.4.1 Step1: Training the team agents\nWe used RLlib Python framework [18] to train the c-MARL agents.\nFor Multiwalker and Pistonball, the multi-agent implementation of\nthe PPO algorithm [30], and for Tag and World Comm, the multi-\nagent APE-X DDPG algorithm [13] were used. The selection of the\nalgorithms was based on the results reported in [36].\n5.4.2 Step2: Training NETij\nThe considered structure for each NETijis shown in Figure 3. In Pis-\ntonball, we used the initial convolutional layers of the agents\u2019 policy\nnetworks, obtained during the c-MARL training stage, as the prepro-\ncessing network. In other environments, the preprocessing network\nconsists of a fully connected layer followed by a ReLU activation,\ntrained from scratch along with the other layers. We provide further\ndetails in Appendix D. We trained NETijover 5000 episodes for\nMultiwalker and Pistonball, and 20,000 episodes for Tag and World\nComm. During the mentioned training phase the team agents used\nthe policies learned in Step 1.\n5.4.3 Step3: Training the Attacks\nWe trained the ACT and the DYN attacks using a single-agent RL\nalgorithm as explained in Section 5.2. We trained multiple DYN at-\ntacks using different \u03bbvalues. For training each attack, we employed\nthe single-agent PPO algorithm in RLlib over 20,000 episodes and\nselected the policy with the best adversarial reward.\n1Code available at https://github.com/kiarashkaz/PGC-Adversarial-\nDetection-Continuous\n\n--- Page 6 ---\n(a) Multiwalker\n (b) Tag\n (c) World Comm\n (d) Pistonball\nFigure 4. ROC of the proposed method against different attacks in all environments.\nTable 2. AUC score of PGC and Discrete against RAND, ACT, GRAD and DYN attacks in all environments.\nMultiwalker Tag World Comm Pistonball\nAttack Types PGC Discrete PGC Discrete PGC Discrete PGC Discrete\nACT 0.996 0.972 0.993 0.948 0.995 0.821 0.999 0.758\nRAND 0.995 0.855 0.843 0.893 0.677 0.713 0.997 0.970\nGRAD 0.674 0.566 0.653 0.858 0.884 0.913 0.581 0.554\nDYN1 0.929 0.818 0.988 0.964 0.992 0.754 0.907 0.711\nDYN2 0.954 0.788 0.968 0.944 0.912 0.707 0.876 0.658\nTable 3. Average total episodic reward under different attacks.\nMultiwalker TagWorld\nCommPistonball\nNo Attack -12.7 101.8 37.6 228.6\nACT -107.6 64.9 26.7 83.1\nRAND -75.6 68.1 30.4 202.1\nGrad -42.7 90.4 34.7 215.1\nDYN1 -96.9 65.1 27.8 95.5\nDYN2 -89.6 69.2 30.1 139.3\n5.4.4 Step4: Evaluation of the Detector\nFor each attack (and also a scenario without any attack), we used the\nc-MARL polices, an attack policy, and a detector for 500 episodes.\nThe detection rule applied in our evaluations was as follows: if any\nof the non-victim agents triggered a detection, we considered the at-\ntack as detected at that particular time step (i.e., u= 1). For the\nevaluation we considered the same set of detection thresholds for all\nagents. The value of hyperparameter wis 0.5 in Multiwalker and\nWorld Comm, 0.75 in Tag and 0.05 in Pistonball. We provide results\nfor other values of win Appendix C.1.\n5.5 Detection Performance\nWe first use the Receiver Operating Characteristic (ROC) curve to\nevaluate the performance of the proposed detector. The ROC curve\nshows the true positive rate against the false positive rate, gener-\nated by varying detection thresholds \u03b2v+and\u03b2v\u2212. The true posi-\ntive rate is defined as the fraction of attacked episodes that are suc-\ncessfully detected, while the false positive rate is the fraction of\nunattacked episodes incorrectly classified as attacked. The detection\nperformance can characterized using the Area Under the ROC Curve\n(AUC), where AUC=1 corresponds to a perfect detector. The ROC\ncurves of all environments is shown in Figure 4.\nFigure 4 and Table 2 show the ROC curves and the AUC scores\nof PGC against different attacks in all environments, respectively.\nDYN1 and DYN2 correspond to two DYN attacks trained with dif-\nferent values of \u03bb, illustrating varying levels of detectability. To il-\nlustrate the corresponding attack impact, Table 3 shows the averagetotal reward collected by the team agents under the different attacks.\nThese results show that PGC can successfully detect the impactful\nattacks in all environments. The ACT attack, which causes the most\nsignificant reduction in team reward (e.g., from -12.7 to -107.6 in\nMultiwalker), can be detected almost perfectly, with an AUC score\nclose to 1. Detecting attacks with lower impact is more challenging.\nFor example, GRAD in Pistonball or RAND in World Comm can\npartially bypass the detector (resulting in low AUC scores), but at the\nsame time, the reduction in the team reward caused by these attacks\nis negligible. One explanation for this observation is that the victim\nagent\u2019s original policy closely resembles these adversarial policies,\nmaking it unsurprising that distinguishing between the agent\u2019s nor-\nmal behavior and these attacks is challenging. The trade-off between\nthe attack impact and the detectability of the attack can be observed\nin other scenarios as well, which confirms that only those attacks can\n(partially) remain undetected that have very limited impact on the\nperformance of the system.\nTable 2 also shows that PGC outperforms Discrete in most cases,\nespecially against the more impactful attacks. Discrete performs bet-\nter than PGC against GRAD in some scenarios. This can be explained\nby the fact that manipulated actions resulting from Grad are actually\nslight variations of the normal actions at each step. Thus they might\nnot trigger a high anomaly score in PGC, but they might correspond\nto a different beam when distributions are modeled over a discrete\nset, resulting in a relatively higher anomaly score.\nAnother key aspect of comparison is complexity. PGC has sig-\nnificantly lower computational cost than Discrete . For example, in\nTag, the detector networks for Discrete require 243 outputs (one per\naction), while PGC requires only 20 outputs (one per action space\ndimension and one per covariance). This difference becomes even\nmore pronounced in high-dimensional environments. For example,\nDiscrete requires neural networks with 39\u22482\u00d7104outputs, while\nthis number is only 54 for PGC. The high output dimensionality of\nDiscrete necessitates significantly larger hidden layers for effective\ntraining, making it extremely computationally demanding to achieve\nperformance comparable to that of PGC.\nFinally, an important metric for evaluating a detector\u2019s perfor-\nmance is its time to detection. We provide the evaluation results for\nPGC in Appendix C.2.\n\n--- Page 7 ---\nTable 4. AUC score of different detector baselines against RAND.\nVariation Multiwalker Tag World Comm\nPGC 0.99 0.84 0.67\nIPGC 0.73 0.53 0.55\nBeta 0.51 0.50 0.50\nDirichlet 0.54 0.50 0.50\n5.6 Why Multivariate Gaussian?\nRecall that PGC uses a multivariate Gaussian distribution with a non-\ndiagonal covariance matrix for the approximation of the action dis-\ntribution. IPGC is a simpler alternative that assumes different dimen-\nsions of aj\ntare independent, and thus, \u03a3ij\ntis diagonal. Hence, NETij\nwould have 2doutputs. While this simplification may not impact per-\nformance in certain scenarios, there are environments where the as-\nsumption of independence of action dimensions does not hold.\nTable 4 shows the AUC scores of PGC and IPGC against RAND\nin environments with an action dimension greater than 1. The re-\nsults indicate significantly lower scores for IPGC, particularly in Tag\nand World Comm, where RAND is almost undetectable by IPGC,\ndespite its impact in Tag. The poor performance of IPGC is due\nto the high correlation between elements of the action vector in\nnon-compromised agents. Training independent predictor networks\nin such cases results in a predicted distribution with high variance,\nmaking the RAND attack undetectable. We note that, however, there\nmight be environments where the elements of the action vector are\nclose to independent, and in such environments IPGC can be used to\nreduce the complexity of detection.\nTable 4 shows the AUC scores obtained using Beta and Dirich-\nlet distributions (which partially account for correlations between di-\nmensions) against RAND. The results shows that neither distribution\nproduces an effective detector, as the RAND attack is nearly unde-\ntected in all scenarios.\nTo further understand how accurate is the Gaussian approxima-\ntion of the distributions fij, we compared the episodic average of\nthe normality score under normal agent behavior with its expected\nvalue under the Gaussian assumption (Appendix A). We found that\nthese quantities are very close in all environments, supporting that\nthe Gaussian approximation may be accurate. Clearly, there may be\nMARL scenarios where fijare multi-modal, in which case a cen-\ntralized approach could be a more effective choice for detection. We\nprovide a more detailed discussion in Appendix A.\n5.7 Parameter Sharing for Reduced Complexity\nSo far, we showed results when each agent has a separate detector for\nkeeping track of every observable neighbor\u2019s action. This requires a\ntotal ofP\ni|Ki|predictor networks to be trained for detection,which\ncan amount to as many as K(K\u22121)networks in the worst case.\nIn the following, we explore whether parameter sharing, a common\napproach in training MARL algorithms [4], [36], can be effectively\napplied in attack detection. With parameter sharing, a single predictor\nnetwork is trained per potential victim agent, and the trained model\nis shared among all agents that observe that neighbor, reducing the\ntotal number of models to K. It is important to note that parameter\nsharing is applicable only to environments where the dimension of\nall agents\u2019 observations is the same, and there exists some form of\nsymmetry among the observation spaces of the agents.\nTable 5 shows the AUC score of PGC trained using parameter\nsharing. Comparing Table 2 and Table 5 shows that parameter shar-\ning does not affect performance. Interestingly, in some cases, param-Table 5. AUC score of PGC trained with parameter sharing against RAND\nand ACT.\nMultiwalker TagWorld\nCommPistonball\nACT 0.995 0.994 0.997 0.999\nRAND 0.995 0.817 0.713 0.997\nTable 6. Average total team reward and AUC scores of PGC when\ndetecting ACT and RAND attacks against two victim agents.\nMultiwalker Pistonball\nAttack Type Reward AUC Reward AUC\nACT -109.6 0.998 75.8 0.997\nRAND -96.0 0.997 180.6 0.983\neter sharing has even enhanced performance. This observation aligns\nwith the findings presented by [36], which demonstrated that param-\neter sharing can enhance the training of MARL algorithms with con-\ntinuous action spaces. The improvement is attributed to parameter\nsharing utilizing more trajectories to train a shared architecture, lead-\ning to potential improvements in sample efficiency.\n5.8 Multiple Victims\nFinally, we consider the case when there are more than one victim\nagents in the environment. Recall that each predictor is trained us-\ning the non-attacked behavior of individual agents. Consequently, the\nnumber of victim agents should not influence the training of the pre-\ndictors. Furthermore, the computation of the normality score for an\nagent\u2019s actions is independent of the actions of other agents. This\nindependence in normality scores for different agents should enable\nPGC to perform well in the presence of multiple victim agents.\nTo illustrate this, we evaluated PGC against two victim agents\nin Multiwalker and Pistonball. We considered that detection occurs\nwhen both victims are detected. Table 6 shows the team reward and\nthe AUC scores obtained. Overall, the results confirm that PGC can\nbe utilized even when there is more than one victim.\n6 Conclusion\nIn this paper, we introduced a decentralized approach for detecting\nadversarial attacks on multi-agent RL systems with continuous ac-\ntion space. Our proposed scheme leverages the observations of the\nagents to predict the action distributions of other agents. We used\nRNNs to approximate these distributions as parameterized Gaussian\ndensities. We put forward a low-complexity anomaly score, com-\nputed by comparing the predictions with the actual actions taken by\nthe agents. We analytically showed that the proposed score has a con-\nstant mean and employed the CUSUM method to detect deviations\nfrom this value. The proposed method was evaluated against vari-\nous attack strategies with different levels of impact and detectabil-\nity. Our results show that the proposed method outperformed its dis-\ncrete counterpart, with only low-impact attacks having the potential\nto evade detection. Moreover, we empirically showed that the non-\ndiagonal elements of the covariance matrix of the parameterized dis-\ntributions used for detection are important for high detection perfor-\nmance. We also found that the computational complexity of train-\ning the detector could be reduced through parameter sharing without\ncompromising the detection rate, and that the proposed detector is\neffective in scenarios with multiple victims.\n\n--- Page 8 ---\nAcknowledgements\nThis work was partly funded by the Swedish Research Council\nthrough projects 2020-03860 and 2024-05269, and by Digital Fu-\ntures through the CLAIRE project. The computations were enabled\nby resources provided by the National Academic Infrastructure for\nSupercomputing in Sweden (NAISS) at Link\u00f6ping University par-\ntially funded by the Swedish Research Council through grant agree-\nment no. 2022-06725.\nReferences\n[1] V . Behzadan and A. Munir. Vulnerability of deep reinforcement learn-\ning to policy induction attacks. In Proc. of International Confer-\nence on Machine Learning and Data Mining (MLDM) , pages 262\u2013275.\nSpringer, 2017.\n[2] A. Bukharin, Y . Li, Y . Yu, Q. Zhang, Z. Chen, S. Zuo, C. Zhang,\nS. Zhang, and T. Zhao. Robust multi-agent reinforcement learning\nvia adversarial regularization: Theoretical foundation and stable algo-\nrithms. Proc. of NeurIPS , 2023.\n[3] L. Canese, G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino,\nM. Re, and S. Span\u00f2. Multi-agent reinforcement learning: A review\nof challenges and applications. Applied Sciences , 11(11), 2021. ISSN\n2076-3417. URL https://www.mdpi.com/2076-3417/11/11/4948.\n[4] F. Christianos, G. Papoudakis, M. A. Rahman, and S. V . Albrecht. Scal-\ning multi-agent reinforcement learning with selective parameter shar-\ning. In Proc. of ICML , pages 1989\u20131998, 2021.\n[5] R. Dadashi, L. Hussenot, D. Vincent, S. Girgin, A. Raichuk, M. Geist,\nand O. Pietquin. Continuous control with action quantization from\ndemonstrations. In Proc. of ICML , pages 4537\u20134557, 2022.\n[6] N. Elhami Fard and R. R. Selmic. Adversarial attacks on heterogeneous\nmulti-agent deep reinforcement learning system with time-delayed data\ntransmission. Journal of Sensor and Actuator Networks , 11(3):45, 2022.\n[7] M. Figura, K. C. Kosaraju, and V . Gupta. Adversarial attacks in\nconsensus-based multi-agent reinforcement learning. In Proc. of ACC ,\npages 3050\u20133055, 2021. doi: 10.23919/ACC50511.2021.9483080.\n[8] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell.\nAdversarial policies: Attacking deep reinforcement learning. In Proc.\nof ICLR , 2019.\n[9] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing\nadversarial examples. In Proc. of ICLR , 2015.\n[10] J. Guo, Y . Chen, Y . Hao, Z. Yin, Y . Yu, and S. Li. Towards comprehen-\nsive testing on the robustness of cooperative multi-agent reinforcement\nlearning. In Proc. of IEEE/CVF CVPR , pages 115\u2013122, 2022.\n[11] J. K. Gupta, M. Egorov, and M. Kochenderfer. Cooperative multi-agent\ncontrol using deep reinforcement learning. In Proc. of AAMAS , 2017.\n[12] T. Haider, K. Roscher, F. Schmoeller da Roza, and S. G\u00fcnnemann. Out-\nof-distribution detection for reinforcement learning agents with proba-\nbilistic dynamics models. In Proc. of AAMAS , pages 851\u2013859, 2023.\n[13] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel,\nH. Van Hasselt, and D. Silver. Distributed prioritized experience replay.\n2018.\n[14] R. A. Horn and C. R. Johnson. Matrix analysis . Cambridge University\nPress, 2012.\n[15] K. Kazari, E. Shereen, and G. D\u00e1n. Decentralized anomaly detection\nin cooperative multi-agent reinforcement learning. In Proc. of IJCAI ,\n2023.\n[16] K. Kazari, A. Kanellopoulos, and G. D\u00e1n. Quickest detection of adver-\nsarial attacks against correlated equilibria. In Proc. of the AAAI Confer-\nence on Artificial Intelligence , 2025.\n[17] E. L. Lehmann, J. P. Romano, and G. Casella. Testing statistical hy-\npotheses , volume 3. Springer, 2005.\n[18] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. Gon-\nzalez, M. Jordan, and I. Stoica. RLlib: Abstractions for distributed re-\ninforcement learning. In Proc. of ICML , pages 3053\u20133062, 2018.\n[19] J. Lin, K. Dzeparoska, S. Q. Zhang, A. Leon-Garcia, and N. Papernot.\nOn the robustness of cooperative multi-agent reinforcement learning. In\nProc. of IEEE Security and Privacy Workshops (SPW) , 2020.\n[20] Y .-C. Lin, M.-Y . Liu, M. Sun, and J.-B. Huang. Detecting adversarial\nattacks on neural network policies with visual foresight. arXiv preprint\narXiv:1710.00814 , 2017.\n[21] C. Lischke, T. Liu, J. McCalmon, M. A. Rahman, T. Halabi, and\nS. Alqahtani. LSTM-based anomalous behavior detection in multi-\nagent reinforcement learning. In Proc. of IEEE CSR , 2022.[22] R. Lowe, Y . Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch.\nMulti-agent actor-critic for mixed cooperative-competitive environ-\nments. Proc. of NeurIPS , 2017.\n[23] P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, and\nG. Shroff. LSTM-based encoder-decoder for multi-sensor anomaly de-\ntection. arXiv preprint arXiv:1607.00148 , 2016.\n[24] K. Mo, W. Tang, J. Li, and X. Yuan. Attacking deep reinforcement\nlearning with decoupled adversarial policy. IEEE Transactions on De-\npendable and Secure Computing , 20(1):758\u2013768, 2022.\n[25] M.-h. Oh and G. Iyengar. Sequential anomaly detection using inverse\nreinforcement learning. In Proc. of ACM KDD , 2019.\n[26] E. S. Page. Continuous inspection schemes. Biometrika , 41(1-2):100\u2013\n115, 06 1954.\n[27] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary. Ro-\nbust deep reinforcement learning with adversarial attacks. In Proc. of\nAAMAS , 2018.\n[28] A. Russo and A. Proutiere. Towards optimal attacks on reinforcement\nlearning policies. In Proc. of ACC , pages 4561\u20134567, 2021.\n[29] A. Sakryukin, C. Ra\u00efssi, and M. Kankanhalli. Inferring dqn structure\nfor high-dimensional continuous control. In Proc. of ICML , 2020.\n[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Prox-\nimal policy optimization algorithms. arXiv preprint arXiv:1707.06347 ,\n2017.\n[31] A. Sedlmeier, R. M\u00fcller, S. Illium, and C. Linnhoff-Popien. Policy\nentropy for out-of-distribution classification. In Proc. of International\nConference on Artificial Neural Networks , pages 420\u2013431, 2020.\n[32] I. Shames, A. M. Teixeira, H. Sandberg, and K. H. Johansson. Dis-\ntributed fault detection for interconnected second-order systems. Auto-\nmatica , 47(12):2757\u20132764, 2011.\n[33] E. Shereen, K. Kazari, and G. D\u00e1n. Adversarial robustness of multi-\nagent reinforcement learning secondary control of islanded inverter-\nbased AC microgrids. In Proc. of IEEE SmartGridComm , 2023.\n[34] B. G. Tekgul, S. Wang, S. Marchal, and N. Asokan. Real-time adversar-\nial perturbations against deep reinforcement learning policies: attacks\nand defenses. In European Symposium on Research in Computer Secu-\nrity, pages 384\u2013404. Springer, 2022.\n[35] J. Terry, B. Black, N. Grammel, M. Jayakumar, A. Hari, R. Sullivan,\nL. S. Santos, C. Dieffendahl, C. Horsch, R. Perez-Vicente, et al. Petting-\nzoo: Gym for multi-agent reinforcement learning. In Proc. of NeurIPS ,\npages 15032\u201315043, 2021.\n[36] J. K. Terry, N. Grammel, A. Hari, L. Santos, and B. Black. Revisiting\nparameter sharing in multi-agent deep reinforcement learning. 2020.\n[37] E. Todorov, T. Erez, and Y . Tassa. MuJoCo: A physics engine for model-\nbased control. In IEEE/RSJ International Conference on Intelligent\nRobots and Systems , pages 5026\u20135033, 2012.\n[38] C. Wang, S. Erfani, T. Alpcan, and C. Leckie. Oil-ad: An anomaly\ndetection framework for sequential decision sequences. arXiv preprint\narXiv:2402.04567 , 2024.\n[39] Z. Wang, Z. Chen, J. Ni, H. Liu, H. Chen, and J. Tang. Multi-scale one-\nclass recurrent neural networks for discrete event sequence anomaly de-\ntection. In Proc. of ACM KDD , pages 3726\u20133734, 2021.\n[40] L. Xie, S. Zou, Y . Xie, and V . V . Veeravalli. Sequential (quickest)\nchange detection: Classical results and new directions. IEEE Journal\non Selected Areas in Information Theory , 2(2):494\u2013514, 2021.\n[41] R. Yan, Y . Wang, Y . Xu, and J. Dai. A multiagent quantum deep rein-\nforcement learning method for distributed frequency control of islanded\nmicrogrids. IEEE Transactions on Control of Network Systems , 9(4):\n1622\u20131632, 2022. doi: 10.1109/TCNS.2022.3140702.\n[42] Z. Yang and H. Nguyen. Recurrent off-policy baselines for memory-\nbased continuous control. In Proc. of NeurIPS Deep Reinforcement\nLearning Workshop , 2021.\n[43] D. Ye, M.-M. Chen, and H.-J. Yang. Distributed adaptive event-\ntriggered fault-tolerant consensus of multiagent systems with general\nlinear dynamics. IEEE Trans. on Cybernetics , 49(3):757\u2013767, 2019.\n[44] L. Yu, Y . Qiu, Q. Yao, Y . Shen, X. Zhang, and J. Wang. Robust com-\nmunicative multi-agent reinforcement learning with active defense. In\nProc. of of the AAAI Conference on Artificial Intelligence , volume 38,\npages 17575\u201317582, 2024.\n[45] H. Zhang, K. Sun, bo xu, L. Kong, and M. M\u00fcller. A distance-based\nanomaly detection framework for deep reinforcement learning. Trans-\nactions on Machine Learning Research (TMLR) , Oct 2024.\n[46] Y . Zhu, Z. Wang, Y . Zhu, C. Chen, and D. Zhao. Discretizing contin-\nuous action space with unimodal probability distributions for on-policy\nreinforcement learning. IEEE Transactions on Neural Networks and\nLearning Systems , 2024.\n\n--- Page 9 ---\nAppendix\nA Accuracy of the Gaussian Assumption\nProposition 1 states that if aj\nt|oi\ntis Gaussian then the normality score\nzij\nthas a known and constant expected value. To see why the Gaus-\nsian assumption may be reasonable, let us assume that the action of\nagent j(policy) is determined by ojandaj|oj\u223cN(\u00b5(oj),\u03a3). Then\nifoj|oiis also Gaussian, then it can be shown that in some scenarios\n(e.g., when \u00b5(oj)is a linear function), aj|oiis Gaussian.\nTo provide an empirical validation, we computed the episodic av-\nerage of zij\ntacross all environments and compared it with its ex-\npected value under the Gaussian assumption. Table 7 shows that the\nempirical average of the normality score under normal agent behav-\nior closely matches its expected value under the Gaussian assump-\ntion. This indicates that the proposed Gaussian approximation is a\nreasonable choice for the considered environments.\nWe acknowledge that there could be environments where the\nGaussian assumption is a poor approximation. For instance, if oj|oi\nis multi-modal (e.g., a mixture model), then aj|oican also follow\na multi-modal distribution and a single Gaussian may not approxi-\nmate the resulting action distribution well. Notably, such a scenario\nimplies that an agent\u2019s observations are not informative about other\nagents\u2019 observations, which would hinder distributed anomaly de-\ntection in such environments. In such environments a centralized ap-\nproach might be needed.\nB MARL Environments\nB.1 Multiwaker\nIn the Multiwalker [11] environment, a set of bipedal robots cooper-\nate to carry a package towards the right side of a terrain. The agents\nobtain a positive reward based on the change in the position of the\npackage and a large negative reward in case the package falls. Each\nwalker applies force to two joints, one in each leg, resulting in a\n4-dimensional action space. The observation of each agent contains\ninformation about its legs and joints, as well as sensor measurements\nFigure 5. An example of one dimensional multi-modal distribution\nMultiwalker TagWorld\nCommPistonball\nE[zij\nt] -2 -2.5 -4.5 -0.5\nAverage zij\nt-2.010 -2.490 -4.441 -0.508\nTable 7. Episodic average and the expected value of normality score under\nthe Gaussian assumption .of the agent vicinity and neighboring agents. This environment is\nparticularly interesting for our problem, due to the noisy nature of\nthe sensor measurements. We considered a scenario with 5 agents\nfor this benchmark.\nB.2 Tag\nTag(Simple Tag) [22] is one of the Multi-agent Particle Environ-\nments (MPE) family. In this environment, a team of three predators\ntry to hunt a single prey. Predators are rewarded based on the number\nof times they can hit the prey in a limited duration (25 time-steps).\nEach agent\u2019s action is a 5-dimensional vector determining how the\nagent moves. Each agent observes the other agents\u2019 velocities and\nrelative positions as part of the observation vector.\nB.3 World Comm\nWorld Comm [22] is another MPE scenario including two competing\ngroups of agents, which we refer to as red and green agents. Green\nagents\u2019 objective is to collect food items and escape from red agents.\nRed agents are rewarded based on the proximity to green agents.\nThere are forests where agents can hide themselves. We considered\na scenario with 4 red agents and one green agent. The red team is\ntrained using c-MARL and the green agent behaves based on a pre-\ndefined policy. One of the red agents is the leader and is capable of\nobserving all agents at all times and can coordinate its team using a 9-\ndimensional action vector. Other agents\u2019 actions are 5-dimensional.\nB.4 Pistonball\nPistonball is one of the Butterfly environments in PettingZoo, built\non visual Atari spaces and powered by the Chipmunk physics engine.\nIn this environment, a group of pistons collaborates to move a ball\nfrom the right side to the left side of the game border using vertical\nmovements. Each piston\u2019s action is a scalar in (-1, 1), representing\nthe amplitude and direction of its vertical movement. The observation\nfor each piston is an RGB image of its surrounding area, including\nneighboring agents. The reward is determined by the extent of the\nball\u2019s movement toward the left side. We considered a scenario with\n10 agents.\nC Other Results\nC.1 Effect of hyperparameter w\nA hyperparameter for CUSUM detection in (6) is w, which controls\nthe sensitivity of the detection. Table 8 shows the AUC score of PGC\nwith different values of w.\nC.2 Time to Detection\nOne important metric for the performance assessment of a detector\nis the time to detection. We define this metric as the average num-\nber of time steps from the start of the attack until it is detected. The\naveraging is taken only over true positive episodes, i.e., episodes in\nwhich a detection happens are considered. Figure 7 shows the time to\ndetect as a function of false positive rate in all environments. The fig-\nure shows that all the impactful attacks (e.g., ACT) can be detected\nby PGC in less than 5 time steps (or even instantaneously in Mul-\ntiwalker) already at a very low false positive rate. Another notable\nobservation is that the time to detect of impactful attacks is not sig-\nnificantly affected by their stealthiness, as it takes almost the same\ntime to detect the ACT and the DYN attacks in most of the scenarios.\n\n--- Page 10 ---\n(a) Multiwalker\n (b) Tag\n (c) World Comm\n (d) Pistonball\nFigure 6. Evaluated MARL Environments\nMultiwalker Tag World Comm Pistonball\nAttacks w= 0 w= 0.5w= 1 w= 0 w= 0.75 w= 1.5w= 0 w= 0.5w= 1 w= 0 w= 0.05 w= 0.4\nACT 0.79 0.99 0.98 0.96 0.99 0.98 0.98 0.99 0.99 0.98 0.99 0.97\nRAND 0.85 0.99 0.55 0.85 0.84 0.86 0.56 0.67 0.63 0.99 0.99 0.99\nGRAD 0.57 0.67 0.68 0.63 0.65 0.64 0.81 0.88 0.78 0.56 0.58 0.52\nDYN1 0.58 0.92 0.91 0.96 0.98 0.98 0.98 0.99 0.98 0.72 0.90 0.8\nDYN2 0.70 0.95 0.91 0.94 0.96 0.96 0.91 0.91 0.90 0.92 0.87 0.67\nTable 8. AUC score of PGC obtained using different values of w.\nC.3 Window-Based Detection\nA variation of our proposed detector replaces CUSUM with a\nwindow-based averaging of the normality score. Table 9 presents the\nAUC scores of this scheme against various attacks across all envi-\nronments. Comparing these results with the AUC scores obtained by\nPGC (Table 2) reveals that CUSUM significantly enhances detection\nperformance overall.\nC.4 Observation Tracking Baseline\nAs another baseline for comparison, we evaluated an anomaly de-\ntection approach based on the sequence of observations. Specifically,\nwe adapted the detection method proposed by [45] to the multi-agent\nsetting, where each agent independently tracks its own observation\nsequence to detect potential anomalies. This approach is justified by\nthe fact that the victim agent\u2019s actions can cause deviations in the\nobservation trajectories of non-victim agents, making it possible for\nthem to detect anomalies affecting the c-MARL team. However, un-\nlike our method, this approach cannot pinpoint the exact source of\nthese abnormalities, i.e., the victim agent.\nIn this method, which we refer to as Observation Tracking , for\neach action type, the mapped feature vectors of observations leading\nto that action are fitted to a multivariate Gaussian distribution. Dur-\ning execution, the minimum Mahalanobis distance between a new\nobservation and any of the Gaussian models is used as the normality\nMultiwalker TagWorld\nCommPistonball\nACT 0.99 0.97 0.97 0.99\nRAND 0.68 0.80 0.55 0.83\nGrad 0.70 0.57 0.87 0.61\nDYN1 0.96 0.97 0.97 0.90\nDYN2 0.89 0.95 0.97 0.81\nTable 9. AUC score of the proposed detector with window-based\ndetection.Multiwalker Tag\nACT 0.61 0.55\nRAND 0.5 0.5\nGrad 0.5 0.5\nDYN1 0.55 0.52\nDYN2 0.52 0.51\nTable 10. AUC score of the observation tracking approach.\nscore. If this score exceeds a predefined threshold, an anomaly is de-\ntected. Since this method is designed for discrete action setups, we\napplied the same quantization technique as in Discrete withNq= 3.\nFor feature selection, we followed the approach in [45], using 30\ncomponents for PCA transformation.\nTable 10 presents the AUC scores of the Observation Tracking\nmethod for detecting various attacks in Multiwalker and Tag. The re-\nsults highlight the limitations of this approach in distributed detection\nof attacks on agents\u2019 actions, emphasizing the necessity of directly\ntracking actions rather than relying solely on observation sequences.\nD Hyperparameters\nD.1 Training Detectors and Attacks\nThe hyperparameters for training detectors are shown in table 11.\nThe value of \u03bbused for training the DYN1 attack in Multiwalker,\nTag, World Comm, and Pistonball, were 5, 1, 0.5, and 1, respectively.\nFor DYN2 attack these values were 10, 2, 5, and 2, respectively.\nD.2 Training c-MARL\nThe parameters we used for training c-MARL policies are summa-\nrized in Table 12.\n\n--- Page 11 ---\n(a) Multiwalker\n (b) Tag\n (c) World Comm\n (d) Pistonball\nFigure 7. Time to detect vs. false positive rate for the proposed method against different attacks\nHyperparameter Value\nUse_rnn True\nhidden_size (Pistonball) 256\nhidden_size (other environments) 128\nbatch_size 20\ninclude_last_act(Multiwalker) True\ninclude_last_act(Tag/World Comm) False\nlr 5e-4\nTable 11. Hyperparameters used for training predictors.\nEnvironment Hyperparameter Value\nMultiwalker Algorithm PPO\ngamma 0.99\nkl_coeff 0.2\nkl_target 0.01\nlambda 1\nlr 5e-5\nuse_lstm True\nlstm_cell_size 128\nmax_seq_len 25\npost_fcnet_activation relu\nvf_share_layers False\nsgd_minibatch_size 128\nTag/World Comm Algorithm APE-X DDPG\ngamma 0.99\nclip_actions False\nactor_hiddens [400, 300]\ncritic_hiddens [400, 300]\nlr 0.0005\nadam_epsilon 1e-8\ntrain_batch_size 512\nuse_lstm False\nvf_share_layers True\ncompress_observations False\npost_fcnet_activation relu\nPistonball Algorithm PPO\ngamma 0.99\nkl_coeff 0.2\nkl_target 0.01\nlambda 1\nlr 5e-5\nuse_lstm False\npost_fcnet_activation relu\nvf_share_layers False\nsgd_minibatch_size 128\nTable 12. Hyperparameters used for training c-MARL.",
  "project_dir": "artifacts/projects/enhanced_cs.LG_2508.15764v1_Distributed_Detection_of_Adversarial_Attacks_in_Mu",
  "communication_dir": "artifacts/projects/enhanced_cs.LG_2508.15764v1_Distributed_Detection_of_Adversarial_Attacks_in_Mu/.agent_comm",
  "assigned_at": "2025-08-22T21:01:10.440586",
  "status": "assigned"
}